{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions to extract zip, and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, \"rt\"):\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, \"rt\")\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting ratings average, user ratings average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9, Defining the data structure to build a model to predict rating based on a latent factor model using the training rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for user, book, rating in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    data.append([user, book, rating])\n",
    "\n",
    "train_data = data[:190000]\n",
    "valid_data = data[190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerBook = defaultdict(list)\n",
    "\n",
    "for user, book, rating in train_data:\n",
    "    ratingsPerUser[user].append(rating)\n",
    "    ratingsPerBook[book].append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_data)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nBooks = len(ratingsPerBook)\n",
    "users = list(ratingsPerUser.keys())\n",
    "books = list(ratingsPerBook.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([int(d[2]) for d in train_data]) / N\n",
    "\n",
    "alpha = ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBiases = defaultdict(float)\n",
    "bookBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "bookGamma = {}\n",
    "\n",
    "K = 2\n",
    "for u in ratingsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "for b in ratingsPerBook:\n",
    "    bookGamma[b] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, book):\n",
    "    if user in userBiases and book in bookBiases:\n",
    "        return (\n",
    "            alpha\n",
    "            + userBiases[user]\n",
    "            + bookBiases[book]\n",
    "            + inner(userGamma[user], bookGamma[book])\n",
    "        )\n",
    "    elif user in bookBiases and book not in bookBiases:\n",
    "        return alpha + userBiases[user]\n",
    "    elif user not in bookBiases and book in bookBiases:\n",
    "        return alpha + bookBiases[book]\n",
    "    else:\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global bookBiases\n",
    "    global userGamma\n",
    "    global bookGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index : index + nUsers]))\n",
    "    index += nUsers\n",
    "    bookBiases = dict(zip(books, theta[index : index + nBooks]))\n",
    "    index += nBooks\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index : index + K]\n",
    "        index += K\n",
    "    for b in books:\n",
    "        bookGamma[b] = theta[index : index + K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a * b for a, b in zip(x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb, data):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(user, book) for user, book, _ in data]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb * userBiases[u] ** 2\n",
    "        for k in range(K):\n",
    "            cost += lamb * userGamma[u][k] ** 2\n",
    "    for b in books:\n",
    "        cost += lamb * bookBiases[b] ** 2\n",
    "        for k in range(K):\n",
    "            cost += lamb * bookGamma[b][k] ** 2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb, data):\n",
    "    unpack(theta)\n",
    "    N = len(data)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dBookBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dBookGamma = {}\n",
    "    for u in ratingsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for b in ratingsPerBook:\n",
    "        dBookGamma[b] = [0.0 for k in range(K)]\n",
    "    for user, book, rating in data:\n",
    "        pred = prediction(user, book)\n",
    "        diff = pred - float(rating)\n",
    "        dalpha += 2 / N * diff\n",
    "        dUserBiases[user] += 2 / N * diff\n",
    "        dBookBiases[book] += 2 / N * diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[user][k] += 2 / N * bookGamma[book][k] * diff\n",
    "            dBookGamma[book][k] += 2 / N * userGamma[user][k] * diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2 * lamb * userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2 * lamb * userGamma[u][k]\n",
    "    for b in bookBiases:\n",
    "        dBookBiases[b] += 2 * lamb * bookBiases[b]\n",
    "        for k in range(K):\n",
    "            dBookGamma[b][k] += 2 * lamb * bookGamma[b][k]\n",
    "    dtheta = (\n",
    "        [dalpha] + [dUserBiases[u] for u in users] + [dBookBiases[b] for b in books]\n",
    "    )\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for b in books:\n",
    "        dtheta += dBookGamma[b]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [float(d[2]) for d in data]\n",
    "train_alwaysPredictMean = [float(alpha) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4744156039668792"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(train_alwaysPredictMean, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.473552588725499\n",
      "MSE = 1.4735223480936397\n",
      "MSE = 1.4733899542381623\n"
     ]
    }
   ],
   "source": [
    "theta = scipy.optimize.fmin_l_bfgs_b(\n",
    "    cost,\n",
    "    [alpha]\n",
    "    + [0.0] * (nUsers + nBooks)\n",
    "    + [random.random() * 0.1 - 0.05 for k in range(K * (nUsers + nBooks))],\n",
    "    derivative,\n",
    "    args=(train_labels, 1, train_data), maxfun = 75, maxiter = 75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4907801137874852"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack(theta[0])\n",
    "valid_labels = [float(d[2]) for d in valid_data]\n",
    "valid_predictions = [prediction(u, b) for u, b, _ in valid_data]\n",
    "MSE(valid_predictions, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the latent-factor model with a lambda of 1, for the validation set we get a MSE of 1.4907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10, the max-min Bias values|Ids of User and Book for the model we trained using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Max.User Bias</th>\n",
       "      <td>u92864068</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min.User Bias</th>\n",
       "      <td>u11591742</td>\n",
       "      <td>-0.001580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max.Book Bias</th>\n",
       "      <td>b76915592</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min.Book Bias</th>\n",
       "      <td>b57299824</td>\n",
       "      <td>-0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID     Value\n",
       "Max.User Bias  u92864068  0.000404\n",
       "Min.User Bias  u11591742 -0.001580\n",
       "Max.Book Bias  b76915592  0.000829\n",
       "Min.Book Bias  b57299824 -0.000272"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ub = (max(userBiases, key=userBiases.get), max(userBiases.values()))\n",
    "min_ub = (min(userBiases, key=userBiases.get), min(userBiases.values()))\n",
    "max_bb = (max(bookBiases, key=bookBiases.get), max(bookBiases.values()))\n",
    "min_bb = (min(bookBiases, key=bookBiases.get), min(bookBiases.values()))\n",
    "\n",
    "summary_table = [max_ub, min_ub, max_bb, min_bb]\n",
    "\n",
    "pd.DataFrame(\n",
    "    summary_table,\n",
    "    columns=[\"ID\", \"Value\"],\n",
    "    index=[\"Max.User Bias\", \"Min.User Bias\", \"Max.Book Bias\", \"Min.Book Bias\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11, Choosing form a range of hyperpataers to tune model to best performance (lowest MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(hyperparameter_list):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global bookBiases\n",
    "    global userGamma\n",
    "    global bookGamma\n",
    "    \n",
    "    alpha = ratingMean\n",
    "    userBiases = defaultdict(float)\n",
    "    bookBiases = defaultdict(float)\n",
    "    userGamma = {}\n",
    "    bookGamma = {}\n",
    "    \n",
    "    K = 2\n",
    "    for u in ratingsPerUser:\n",
    "        userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    for b in ratingsPerBook:\n",
    "        bookGamma[b] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    \n",
    "    training_MSE = []\n",
    "    valid_MSE = []\n",
    "    for lamb in hyperparameter_list:\n",
    "        print(\"For hyperparameter\")\n",
    "        theta = scipy.optimize.fmin_l_bfgs_b(\n",
    "            cost,\n",
    "            [alpha] + [0.0] * (nUsers + nBooks) + [random.random() * 0.1 - 0.05 for k in range(K * (nUsers + nBooks))],\n",
    "            derivative,\n",
    "            args=(train_labels, lamb, train_data), maxfun = 75, maxiter = 75\n",
    "        )\n",
    "        unpack(theta[0])\n",
    "        training_MSE.append(theta[1])\n",
    "        valid_labels = [float(d[2]) for d in valid_data]\n",
    "        valid_predictions = [prediction(u, b) for u, b, _ in valid_data]\n",
    "        valid_MSE.append(MSE(valid_predictions, valid_labels))\n",
    "    MSE_table = {\n",
    "        \"Hyperparameter\": hyperparameter_list,\n",
    "        \"Training MSE\": training_MSE,\n",
    "        \"Validation MSE\": valid_MSE,\n",
    "    }\n",
    "    return pd.DataFrame(MSE_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hyperparameter\n",
      "MSE = 1.473542867684955\n",
      "MSE = 1.4707492784354008\n",
      "MSE = 1.524751077429654\n",
      "MSE = 1.664430648359679\n",
      "MSE = 1.4571072357488954\n",
      "MSE = 1.458424836696519\n",
      "MSE = 1.4584194341918297\n",
      "MSE = 1.4584193576772098\n",
      "MSE = 1.4584210732917318\n",
      "MSE = 1.4584224633044278\n",
      "For hyperparameter\n",
      "MSE = 1.4735559183679785\n",
      "MSE = 1.4735244758416304\n",
      "MSE = 1.4733899552153853\n",
      "For hyperparameter\n",
      "MSE = 1.473558912755138\n",
      "MSE = 1.4735545906277125\n",
      "MSE = 1.4735459256151198\n",
      "MSE = 1.473545925599727\n",
      "MSE = 1.4735459255395476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameter</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.465883</td>\n",
       "      <td>1.478535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.473469</td>\n",
       "      <td>1.490780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.00</td>\n",
       "      <td>1.473547</td>\n",
       "      <td>1.490908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hyperparameter  Training MSE  Validation MSE\n",
       "0            0.01      1.465883        1.478535\n",
       "1            1.00      1.473469        1.490780\n",
       "2          100.00      1.473547        1.490908"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_list = [0.01, 1, 100]\n",
    "\n",
    "parameter_tuning(hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for the training and validation set is generalized at lambda value 0.00001 from the different training lambda performances summarized above. Using this lambda, model we predict on test set. The solution has been uploaded to kaggle (Username: mouserat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", \"w\")\n",
    "\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    predictions.write(u + \"-\" + b + \",\" + str(prediction(u, b)) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
